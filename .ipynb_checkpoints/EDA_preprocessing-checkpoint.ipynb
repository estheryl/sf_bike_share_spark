{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from pyspark.sql.functions import udf, to_date, to_utc_timestamp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading trip.csv from s3 cluster\n",
    "trip_rdd = sc.textFile('s3://msds694.proj/data/trip.csv', 24)\n",
    "trips_rdd = trip_rdd.map(lambda x: x.split(\",\"))\n",
    "trips = trips_rdd.collect()\n",
    "\n",
    "# Removing the header row from the data\n",
    "header = trips_rdd.first()\n",
    "trips_rdd_rows = trips_rdd.filter(lambda line: line != header)\n",
    "\n",
    "# Creating an RDD with only Subscriber data\n",
    "trips_rdd_rows_subscribers = trips_rdd_rows.filter(lambda x: x[9] != 'Customer')\n",
    "\n",
    "# Creating an RDD with only Customer data\n",
    "trips_rdd_rows_customers = trips_rdd_rows.filter(lambda x: x[9] != 'Subscriber')\n",
    "\n",
    "# Getting the day of the week from the date column\n",
    "# Creating a key-value pair with day of the week as the key, and 1 as the value (to keep track of bike trips)\n",
    "trips_subscriber_dow_subtype = trips_rdd_rows_subscribers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), 1))\n",
    "trips_customer_dow_subtype = trips_rdd_rows_customers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), 1))\n",
    "\n",
    "# GroupByKey and sum of values will fetch the number of trips on a given day of week\n",
    "# For Subscribers:\n",
    "trips_sub_dow_subtype_grouped = trips_subscriber_dow_subtype.groupByKey().mapValues(lambda x: sum(x))\n",
    "# For Customers:\n",
    "trips_cus_dow_subtype_grouped = trips_customer_dow_subtype.groupByKey().mapValues(lambda x: sum(x))\n",
    "# Save for EDA\n",
    "trips_sub_dow_subtype_grouped.saveAsTextFile('s3://msds694.proj/data/sub_total_trip_by_dow')\n",
    "trips_cus_dow_subtype_grouped.saveAsTextFile('s3://msds694.proj/data/cus_total_trip_by_dow')\n",
    "\n",
    "# Getting the day of the week from the date column\n",
    "# Creating a key-value pair with day of the week as the key, and duration of the trip as the value\n",
    "trips_sub_dow_duration = trips_rdd_rows_subscribers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), int(x[1])))\n",
    "trips_cus_dow_duration = trips_rdd_rows_customers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), int(x[1])))\n",
    "\n",
    "# GroupByKey and sum of values will fetch the total duration of trips (in mins) on a given day of week\n",
    "# For Subscribers:\n",
    "trips_sub_dow_duration_sum = trips_sub_dow_duration.groupByKey().mapValues(lambda x: round(sum(x)/60, 2))\n",
    "# For Customers:\n",
    "trips_cus_dow_duration_sum = trips_cus_dow_duration.groupByKey().mapValues(lambda x: round(sum(x)/60, 2))\n",
    "\n",
    "# Getting the day of the week from the date column\n",
    "# Creating a key-value pair with day of the week as the key, and id of the trip as the value\n",
    "trips_sub_dow_id = trips_rdd_rows_subscribers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), x[0]))\n",
    "trips_cus_dow_id = trips_rdd_rows_customers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), x[0]))\n",
    "\n",
    "# GroupByKey and count of ids will fetch the total trips on a given day of week\n",
    "# For Subscribers:\n",
    "trips_sub_dow_id_count = trips_sub_dow_id.groupByKey().mapValues(lambda x: len(set(x)))\n",
    "# For Customers:\n",
    "trips_cus_dow_id_count = trips_cus_dow_id.groupByKey().mapValues(lambda x: len(set(x)))\n",
    "\n",
    "# Join the RDD with (Day of week, total trip duration) as key-value pair, to the\n",
    "# RDD with (Day of week, total trips) as key-value pair on key -> Day of Week\n",
    "trips_sub_dow_id_ct_dur = trips_sub_dow_duration_sum.join(trips_sub_dow_id_count)\n",
    "trips_cus_dow_id_ct_dur = trips_cus_dow_duration_sum.join(trips_cus_dow_id_count)\n",
    "# Compute the average duration per trip per day of week for both Subscribers and the Customers\n",
    "trips_sub_dow_avg_dur = trips_sub_dow_id_ct_dur.mapValues(lambda x: x[0]/x[1])\n",
    "trips_cus_dow_avg_dur = trips_cus_dow_id_ct_dur.mapValues(lambda x: x[0]/x[1])\n",
    "# Save for EDA\n",
    "trips_sub_dow_avg_dur.saveAsTextFile('s3://msds694.proj/data/sub_avg_trip_duration_by_dow')\n",
    "trips_cus_dow_avg_dur.saveAsTextFile('s3://msds694.proj/data/cus_avg_trip_duration_by_dow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1= sc.textFile('s3://msds694.proj/data/trip.csv',8)\n",
    "rdd1.cache()\n",
    "rdd1.collect()\n",
    "lines1 = rdd1.map(lambda x: ((x.split(',')[2][:x.split(',')[2].find(' ')],x.split(',')[9]),\n",
    "                            (x.split(',')[0],x.split(',')[1])))\n",
    "lines1.cache()\n",
    "lines1.collect()\n",
    "pair1 = lines1.groupByKey().mapValues(lambda x:len(list(x)))\n",
    "pair1.cache()\n",
    "pair1.collect()\n",
    "pair2 = pair1.map(lambda x:(x[0][0],(x[0][1],x[1])))\n",
    "pair2.collect()\n",
    "rdd = sc.textFile('s3://msds694.proj/data/weather.csv',24)\n",
    "rdd.collect()\n",
    "lines2 = rdd.map(lambda x: (x.split(',')[0],\n",
    "                            (x.split(',')[2],x.split(',')[19],x.split(',')[23])))\n",
    "lines2.collect()\n",
    "lines3 = lines2.filter(lambda x:x[1][2]=='94107')\n",
    "joined = lines3.leftOuterJoin(pair2)\n",
    "joined.collect()\n",
    "temp = joined.map(lambda x:((x[1][0][0], x[1][1][0] ),x[1][1][1]))\n",
    "rain = joined.map(lambda x:((x[1][0][1], x[1][1][0] ),x[1][1][1]))\n",
    "temp_mean = temp.groupByKey().mapValues(lambda x:(len(list(x)),sum(list(x))/len(list(x))))\n",
    "rain_mean = rain.groupByKey().mapValues(lambda x:(len(list(x)),sum(list(x))/len(list(x))))\n",
    "temp_mean.saveAsTextFile(\"s3://msds694.proj/data/Esther/temp\")\n",
    "rain_mean.saveAsTextFile(\"s3://msds694.proj/data/Esther/rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = sc.textFile('s3://msds694.proj/data/station.csv',24)\n",
    "station_c=station.map(lambda x:x.split(',')[0:4])\n",
    "trip_c=trip.map(lambda x:x.split(',')[1:8])\n",
    "key_pair=trip_c.map(lambda x: ((x[3],x[6]),1))\n",
    "start_end_count=key_pair.groupByKey().mapValues(lambda x : sum(x))\n",
    "start_end_count=start_end_count.map(lambda x: (x[0][0],x[0][1],x[1]))\n",
    "station_pair=station_c.map(lambda x: (x[0],(x[1],x[2],x[3])))\n",
    "start_key=start_end_count.map(lambda x:(x[0],(x[1],x[2])))\n",
    "df1=start_key.join(station_pair).map(lambda x: (x[0],x[1][0][0],x[1][0][1],x[1][1][0],x[1][1][1],x[1][1][2]))\n",
    "df2=df1.map(lambda x: (x[1],(x[0],x[2],x[3],x[4],x[5])))\n",
    "df_full=df2.join(station_pair).map(lambda x:(x[0],x[1][0][0],x[1][0][1],x[1][0][2],\n",
    "                                     x[1][0][3],x[1][0][4],x[1][1][0],x[1][1][1],x[1][1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('s3://msds694.proj/data/trip.csv', 24)\n",
    "rdd = rdd.flatMap(lambda x : [x.replace(';','').split(\",\")])\n",
    "rdd = rdd.filter(lambda x: x[0] != 'id')\n",
    "# subscriber\n",
    "subscriber_1 = rdd.filter(lambda x: x[9] == 'Subscriber')\n",
    "subscriber_1 = subscriber_1.map(lambda x: float(x[1])/60)\n",
    "subscriber_1 = subscriber_1.filter(lambda x: x<60)\n",
    "# customer\n",
    "customer_1 = rdd.filter(lambda x: x[9] == 'Customer')\n",
    "customer_1 = customer_1.map(lambda x: float(x[1])/60)\n",
    "customer_1 = customer_1.filter(lambda x: x<60)\n",
    "# save to S3\n",
    "subscriber_1.saveAsTextFile(\"s3://msds694.proj/data/Lexie/subscriber\")\n",
    "customer_1.saveAsTextFile(\"s3://msds694.proj/data/Lexie/customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_path = 's3://msds694.proj/data/station.csv'\n",
    "status_path = 's3://msds694.proj/data/status.csv'\n",
    "trip_path = 's3://msds694.proj/data/trip.csv'\n",
    "weather_path = 's3://msds694.proj/data/weather.csv'\n",
    "status_rdd = sc.textFile(status_path)\n",
    "station_rdd = sc.textFile(station_path)\n",
    "status_header = status_rdd.take(1)\n",
    "station_header = station_rdd.take(1)\n",
    "status_header,station_header\n",
    "STATION_ID = 0\n",
    "BIKES_AVAILABLE = 1\n",
    "DOCKS_AVAILABLE = 2\n",
    "TIME = 3\n",
    "def change_col(x,col_num,func,**kwargs):\n",
    "    x[col_num] = func(x[col_num],**kwargs)\n",
    "    return x\n",
    "def makekey(x,*args):\n",
    "    key = tuple([item for idx,item in enumerate(x) if idx in args])\n",
    "    return (key,x)\n",
    "def flatten(x,iterations=1):\n",
    "    return sum(x[1])\n",
    "status_rdd_processed = (status_rdd.filter(lambda x: not x.startswith('station_id'))\n",
    "                                  .map(lambda x:x.split(','))\n",
    "                                  .map(lambda x:x[:TIME]+[x[TIME].replace('/','-')])\n",
    "                                  .map(lambda x: change_col(x,TIME,parse))\n",
    "                                  .filter(lambda x: x[TIME] >= datetime(2013,12,29) and x[TIME] <= datetime(2015,1,1))\n",
    "                                  .map(lambda x:x+[datetime(x[TIME].year,x[TIME].month,x[TIME].day,x[TIME].hour)]))\n",
    "status_rdd_processed.cache().count()\n",
    "status_rdd_processed.take(1)\n",
    "status_rdd_proccessed_key = status_rdd_processed.map(lambda x: makekey(x,STATION_ID))\n",
    "station_rdd_processed = (station_rdd.filter(lambda x: not x.startswith('id'))\n",
    "                                    .map(lambda x:x.split(','))\n",
    "                                    .filter(lambda x: x[5] == 'San Francisco')\n",
    "                                    .map(lambda x:makekey(x,0)))\n",
    "status_rdd_join_sf_station = status_rdd_proccessed_key.join(station_rdd_processed)\\\n",
    "                                                      .map(lambda x: x[1][0]+ x[1][1] ).cache()\n",
    "status_rdd_processed_gb_id_hour = status_rdd_join_sf_station.map(lambda x: makekey(x,4,6))\n",
    "min_timestamp_per_hour = status_rdd_processed_gb_id_hour.map(lambda x: (x[0],x[1][3]))\\\n",
    "                                                        .reduceByKey(lambda x,y:min(x,y)).cache()\n",
    "df = status_rdd_processed_gb_id_hour.join(min_timestamp_per_hour)\\\n",
    "                               .map(lambda x: x[1][0] + [x[1][1]])\\\n",
    "                               .filter(lambda x:x[3] == x[12])\\\n",
    "                               .map(lambda x:x[0:12]).cache()\n",
    "def is_weekend(date):\n",
    "    weekday_name = date.strftime('%A')\n",
    "    if weekday_name in ['Saturday','Sunday']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "station_name_key = station_rdd_processed.map(lambda x:x[1])\\\n",
    "                                        .map(lambda x:makekey(x,1))\n",
    "df_agg = (df.map(lambda x: x+[is_weekend(x[4])])          # create weekend flag\n",
    "               .map(lambda x: x+[int(x[1])/int(x[9])])    # calculate percentate available\n",
    "               .map(lambda x: x+[x[4].hour])              # compute hour\n",
    "               .map(lambda x: [x[14],x[12],x[6],x[13]])   # select hour,weekend_flag,station_name,percent_bikes_avail\n",
    "               .map(lambda x: makekey(x,0,1,2))           # make hour,weekend_flag,and station_name keys\n",
    "               .map(lambda x:(x[0],x[1][-1]))             # grab only percent_bikes_avail column to be in values\n",
    "               .aggregateByKey((0,0), lambda a,b: (a[0] + b, a[1] + 1),  \n",
    "                                       lambda a,b: (a[0] + b[0], a[1] + b[1]))  # calculate a tuple of (cum_sum, count) to compute average\n",
    "               .mapValues(lambda x:x[0]/x[1])       # compute average pct_avail for each key\n",
    "               .map(lambda x: list(x[0])+[x[1]])    # flatten key value pairs\n",
    "               .map(lambda x: makekey(x,2))         # make station name the key\n",
    "               .join(station_name_key)              # join with station table to get info about station\n",
    "               .map(lambda x: x[1][0]+x[1][1]))     # flatten after join\n",
    "def toCSVLine(data):\n",
    "  return ','.join(str(d) for d in data)\n",
    "lines = df_agg.map(toCSVLine)\n",
    "lines.saveAsTextFile('s3://msds694.proj/data/marine/test_data_kevin_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
